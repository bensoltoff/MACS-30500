---
title: "Scraping web pages"
date: 2019-03-01

type: book
toc: true
draft: false
aliases: ["/webdata005_scraping.html", "/notes/web-scraping/"]
categories: ["webdata"]

weight: 65
---

```{r}
#| label = "setup",
#| include = FALSE
knitr::opts_chunk$set(cache = TRUE)
```

```{r}
#| label = "packages",
#| cache = FALSE,
#| message = FALSE,
#| warning = FALSE
library(tidyverse)
library(rvest)
library(lubridate)

theme_set(theme_minimal())
```

{{% callout note %}}

Run the code below in your console to download this exercise as a set of R scripts.

```r
usethis::use_course("cis-ds/getting-data-from-the-web-scraping")
```

{{% /callout %}}

What if data is present on a website, but isn't provided in an API at all? It is possible to grab that information too. How easy that is to do depends a lot on the quality of the website that we are using.

## HTML

HTML is a structured way of displaying information. It is very similar in structure to XML (in fact many modern html sites are actually XHTML5, [which is also valid XML](http://www.w3.org/TR/html5/the-xhtml-syntax.html))

{{< figure src="https://imgs.xkcd.com/comics/tags.png" caption="[tags](https://xkcd.com/1144/)" >}}

## Process

**HyperText Markup Language** (HTML) is the basic building block of the World Wide Web. It defines the structure and format of content on web pages. The HTML code is stored on a server and retrieved by your computer when you visit a web page.

1. The web browser sends a request to the server that hosts the website.
1. The server sends the browser an HTML document.
1. The browser uses instructions in the HTML to render the website.

## Components of HTML code

HTML code looks something like this:

```html
<html>
  <head>
    <title>Title</title>
    <link rel="icon" type="icon" href="http://a" />
    <link rel="icon" type="icon" href="http://b" />
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
      <tr>
        <td>Idina</td>
        <td>Menzel</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```

HTML code consists of **markup** code used to annotate text, images, and other content for display in a web browswer. As you can see, the code above consists of HTML **elements** which are created by a tag `<>`. Elements can also have **attributes** that configure the elements or adjust their behavior.

{{% callout note %}}

You can think of elements as R functions, and attributes are the arguments to functions. Not all functions require arguments, or they use default arguments.

{{% /callout %}}

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - attribute (argument)
* `"http://github.com"` - attribute (value)
* `GitHub` - content

HTML code utilizes a nested structure. The above tags can be represented as:

* `html`
    * `head`
        * `title`
        * `link`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
            * `tr`
                * `td`
                * `td`
        * `img`

Let's say we want to find the content "here". Which tag in our sample HTML code contains that content?

* `html`
    * `head`
        * `title`
        * `link`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * <span style="color:red">**`b`**</span>
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
            * `tr`
                * `td`
                * `td`
        * `img`

## CSS selectors

**Cascading Style Sheets** (CSS) are a flexible framework for customizing the appearance of elements in a web page. They work in conjunction with HTML to format the appearance of content on the web.

##### HTML

{{< figure src="shiny-css-none.png" caption="HTML only" >}}

##### HTML + CSS

{{< figure src="shiny-css.png" caption="HTML + CSS" >}}

## CSS code

```css
span {
  color: #ffffff;
}

.num {
  color: #a8660d;
}

table.data {
  width: auto;
}

#firstname {
  background-color: yellow;
}
```

CSS uses **selectors**  and **styles**. Selectors define to which elements of the HTML code the styles apply. A CSS script describes an element by its **tag**, **class**, and/or **ID**. Class and ID are defined in the HTML code as attributes of the element.

```html
<span class="bigname" id="shiny">Shiny</span>
```

* `<span></span>` - tag name
* `bigname` - class (optional)
* `shiny` - id (optional)

So a CSS selector of

```css
span
```

would select all elements with the `span` tag. Likewise, a CSS selector of

```css
.bigname
```

selects all elements with the `bigname` class (note the use of a `.` to select based on class). A CSS selector of

```css
span.bigname
```

selects all elements with the `span` tag **and** the `bigname` class. Finally,

```css
#shiny
```

selects all elements with the `shiny` id.

Prefix | Matches
-------|--------
none   | tag
.      | class
#      | id

{{% callout note %}}

[CSS diner](http://flukeout.github.io) is a JavaScript-based interactive game for learning and practicing CSS selectors. Take some time to play and learn more about CSS selector combinations.

{{% /callout %}}

## Find the CSS selector

```html
<body>
    <table id="content">
        <tr class='name'>
            <td class='firstname'>
                Kurtis
            </td>
            <td class='lastname'>
                McCoy
            </td>
        </tr>
        <tr class='name'>
            <td class='firstname'>
                Leah
            </td>
            <td class='lastname'>
                Guerrero
            </td>
        </tr>
    </table>
</body>
```

Find the CSS selectors for the following elements in the HTML above:

1. The entire table.
1. Just the element containing first names.

(Hint: There will be multiple solutions for each.)

{{< spoiler text="Click for the solution" >}}

1. Options include

    ```css
    table
    #content
    table#content
    ```
1. Options include

    ```css
    .firstname
    td.firstname
    .name .firstname
    tr .firstname
    ```
    
{{< /spoiler >}}

## Scraping presidential statements

To demonstrate webscraping in R, we are going to collect records on presidential statements from [The American Presidency Project](https://www.presidency.ucsb.edu/).

Let's say we are interested in how presidents speak about "space exploration." On the website, we punch in this search term, and we get the [following 346 results](https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100).[^count]

Our goal is to scrape these records and store pertinent information in a data frame. We will be doing this in two steps:

1. Write a function to scrape each individual record page.
2. Use this function to loop through all results, and collect all pages.

Load the following packages to get started:

```r
library(tidyverse)
library(rvest)
library(lubridate)
```

### Using `rvest` to read HTML

The package [`rvest`](https://rvest.tidyverse.org/) allows us to:

1. Collect the HTML source code of a webpage.
2. Read the HTML of the page.
3. Select and keep certain elements of the page that are of interest.

Let's start with step one. We use the `read_html` function to call the results URL and grab the HTML response. Store this result as an object.

```{r}
#| label = "get-statement"
dwight <- read_html(x = "https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration")

# Let's take a look at the object we just created
dwight
```

This is pretty messy. We need to use `rvest` to make this information more usable.

### Find page elements

`rvest` has a number of functions to find information on a page. Like other webscraping tools, `rvest` lets you find elements by their:

1. HTML tags.
1. HTML attributes.
1. CSS selectors.

Let's search first for HTML tags.

The function `html_elements` searches a parsed HTML object to find all the elements with a particular HTML tag, and returns all of those elements.

What does the example below do?

```{r}
#| label = "nodes",
#| dependson = "get-statement"
html_elements(x = dwight, css = "a")
```

That is a lot of results! Many elements on a page will have the same HTML tag. For instance, if you search for everything with the `a` tag, you are likely to get a lot of stuff, much of which you do not want. 

In our case, we only want the links corresponding to the speaker Dwight D. Eisenhower.

{{< figure src="scraping-links.png" caption="Special Message to the Congress Relative to Space Science and Exploration." >}}

### SelectorGadget

**SelectorGadget** is a GUI tool used to identify CSS selector combinations from a webpage.

#### Install SelectorGadget

1. Read [here](https://rvest.tidyverse.org/articles/articles/selectorgadget.html)
1. Drag **SelectorGadget** link into your browser's bookmark bar

#### Using SelectorGadget

1. Navigate to a webpage
1. Open the SelectorGadget bookmark
1. Click on the item to scrape
1. Click on yellow items you do not want to scrape
1. Click on additional items that you do want to scrape
1. Rinse and repeat until only the items you want to scrape are highlighted in yellow
1. Copy the selector to use with `html_elements()`

{{% callout note %}}

When using SelectorGadget, always make sure to scroll up and down the web page to make sure you have properly selected only the content you want.

{{% /callout %}}

#### Find the CSS selector

Use Selector Gadget to find the CSS selector for the document's *speaker*.

Then, modify an argument in `html_elements` to look for this more specific CSS selector.

{{< spoiler text="Click for the solution" >}}

```{r}
#| label = "get-speaker",
#| dependson = "get-statement"
html_elements(x = dwight, css = ".diet-title a")
```

{{< /spoiler >}}

### Get attributes and text of elements

Once we identify elements, we want to access information in those elements. Oftentimes this means two things:

1. Text
1. Attributes

Getting the text inside an element is pretty straightforward. We can use the `html_text2()` command inside of `rvest` to get the text of an element:

```{r}
#| label = "get-speaker-text",
#| dependson = "get-statement"
# identify element with speaker name
speaker <- html_elements(dwight, ".diet-title a") %>% 
  html_text2() # Select text of element

speaker
```

You can access a tag's attributes using `html_attr`. For example, we often want to get a URL from an `a` (link) element. This is the URL the link "points" to. It is contained in the attribute `href`:

```{r}
speaker_link <- html_elements(dwight, ".diet-title a") %>% 
  html_attr("href")

speaker_link
```

### Let's do this!

Believe it or not, that is all you need to scrape a website. Let's apply those skills to scrape a sample document from the UCSB website -- the [first item in our search results](https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration). 

We will collect the document's date, speaker, title, and full text.

**Think**: Why are we doing through all this effort to scrape just one page?

1. Date

    ```{r}
    #| label = "get-date",
    #| dependson = "get-statement"
    date <- html_elements(x = dwight, css = ".date-display-single") %>%
      html_text2() %>% # Grab element text
      mdy() # Format using lubridate
    date
    ```

1. Speaker

    ```{r}
    #| label = "get-speaker-2",
    #| dependson = "get-statement"
    speaker <- html_elements(x = dwight, css = ".diet-title a") %>%
      html_text2()
    speaker
    ```
    
1. Title

    ```{r}
    #| label = "get-title",
    #| dependson = "get-statement"
    title <- html_elements(x = dwight, css = "h1") %>%
      html_text2()
    title
    ```

1. Text

    ```{r}
    #| label = "get-text",
    #| dependson = "get-statement"
    text <- html_elements(x = dwight, css = "div.field-docs-content") %>%
      html_text2()
    
    # This is a long document, so let's just display the first 1,000 characters
    text %>% str_sub(1, 1000) 
    ```
    
#### Make a function

Make a function called `scrape_docs` that accepts a URL of an individual document, scrapes the page, and returns a data frame containing the document's date, speaker, title, and full text.

This involves:

- Requesting the HTML of the webpage using the full URL and `rvest`
- Using `rvest` to locate all elements on the page we want to save
- Storing each of those items into a data frame
- Returning that data frame

```r
scrape_doc <- function(url){

  # YOUR CODE HERE
  
}

# Uncomment to test
# scrape_doc("https://www.presidency.ucsb.edu/documents/letter-t-keith-glennan-administrator-national-aeronautics-and-space-administration")
```

{{< spoiler text="Click for the solution" >}}

```{r}
#| label = "scrape-function"
scrape_doc <- function(url){
  # get HTML page
  url_contents <- read_html(x = url)
  
  # extract elements we want
  date <- html_elements(x = url_contents, css = ".date-display-single") %>%
    html_text2() %>% # Grab element text
    mdy() # Format using lubridate
  
  speaker <- html_elements(x = url_contents, css = ".diet-title a") %>%
    html_text2()
  
  title <- html_elements(x = url_contents, css = "h1") %>%
    html_text2()
  
  text <- html_elements(x = url_contents, css = "div.field-docs-content") %>%
    html_text2()
  
  # store in a data frame
  url_data <- tibble(
    date = date,
    speaker = speaker,
    title = title,
    text = text
  )
  
  # return the data frame
  return(url_data)
}

scrape_doc("https://www.presidency.ucsb.edu/documents/letter-t-keith-glennan-administrator-national-aeronautics-and-space-administration")
```

{{< /spoiler >}}

## Scrape cost of living data

Look up the cost of living for Ithaca, NY on [Sperling's Best Places](http://www.bestplaces.net/). Then extract it with `html_elements()` and `html_text()`.

{{< spoiler text="Click for the solution" >}}

We need to obtain information from [Ithaca, NY](https://www.bestplaces.net/cost_of_living/city/new_york/ithaca).

```{r}
#| label = "ithaca"
ithaca <- read_html("https://www.bestplaces.net/cost_of_living/city/new_york/ithaca")

col <- html_elements(ithaca, css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)")
html_text2(col)

# or use a piped operation
ithaca %>%
  html_elements(css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)") %>%
  html_text2()
```

{{< /spoiler >}}

## Tables

Use `html_table()` to scrape whole tables of data as a data frame.

```{r}
#| label = "ithaca-table",
#| dependson = "ithaca"
tables <- html_elements(ithaca, css = "table")

tables %>%
  # get the first table
  nth(1) %>%
  # convert to data frame
  html_table(header = TRUE)
```

## Extract climate statistics

Visit the climate tab for your home town. Extract the climate statistics of your hometown as a data frame with useful column names.

{{< spoiler text="Click for the solution" >}}

We can find the climate information for Ithaca, NY [here](http://www.bestplaces.net/climate/city/new_york/ithaca).

```{r}
#| label = "ithaca-climate"
ithaca_climate <- read_html("http://www.bestplaces.net/climate/city/new_york/ithaca")

climate <- html_elements(ithaca_climate, css = "table")
html_table(climate, header = TRUE, fill = TRUE)[[1]]

ithaca_climate %>%
  html_elements(css = "table") %>%
  nth(1) %>%
  html_table(header = TRUE)
```

{{< /spoiler >}}

## Acknowledgments

* Scraping presidential statements drawn from [PLSC 31101: Computational Tools for Social Science](https://plsc-31101.github.io/course/collecting-data-from-the-web.html#webscraping)
* [HTML| Mozilla Developer Network](https://developer.mozilla.org/en-US/docs/Web/HTML)
* [CSS | Mozilla Developer Network](https://developer.mozilla.org/en-US/docs/Web/CSS)

## Session Info

```{r}
#| child = here::here("R", "_session-info.Rmd")
```

[^count]: 372 results as of July 13, 2021.
